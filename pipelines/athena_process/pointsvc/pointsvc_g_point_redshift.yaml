# NOTE: 2 hours delayed dag
---
pipeline_key: pointsvc_g_point_redshift
pipeline_type: athena_process
pipeline_dag_configs:
  start_date: 2020-04-29 04:00:00
  schedule_interval: "0 * * * *"

alerts:
  slack:
    - trigger: failure
      args:
        channel: data-emergency
    - trigger: sla_miss
      args:
        channel: data-emergency
    - trigger: retry
      args:
        channel: data-warning

upstream_dependencies:
  - dag_id: athena_view_pointsvc_g_vw_point
    timedelta_hours: 2

downstream_dependencies:
  - dag_id: redshift_query_pointsvc_piont
    task_id: run_redshift_query

sensor_soft_fail: false

athena:
  workgroup: pointsvc
  # Below time configs are needed to set time prefix for S3 directory.
  year:  "{{ (execution_date - macros.timedelta(hours=2)).strftime('%Y') }}"
  month: "{{ (execution_date - macros.timedelta(hours=2)).strftime('%m') }}"
  day:   "{{ (execution_date - macros.timedelta(hours=2)).strftime('%d') }}"
  hour:  "{{ (execution_date - macros.timedelta(hours=2)).strftime('%H') }}"

  process_query: |
    CREATE TABLE IF NOT EXISTS {database}.{temp_table}
    WITH (
          format = 'PARQUET',
          parquet_compression = 'SNAPPY',
          external_location = 's3://{output_bucket}/{output_prefix}'
    ) AS (
        SELECT
          resource_id,
          resource_type,
          event_type,
          redeem_id,
          redeem_type,
          redeem_status,
          amount,
          deposit_sum,
          withdraw_sum,
          account_id,
          app_id,
          publisher_user_id,
          reward_transaction_id,
          unit_id,
          extra,
          created_at,
          updated_at,
          version,
          scatter
        from
          {{{{ var.value.get('server_env', 'prod') }}}}_pointsvc.g_vw_point
        WHERE
          partition_timestamp >= TIMESTAMP'{{{{ (execution_date - macros.timedelta(hours=2)).strftime('%Y-%m-%d %H:00:00') }}}}' AND
          partition_timestamp < TIMESTAMP'{{{{ (next_execution_date - macros.timedelta(hours=2)).strftime('%Y-%m-%d %H:00:00') }}}}'
    );
  output_bucket: "{{ var.value.get('server_env', 'prod') }}-buzzvil-data-lake"
  output_prefix: pointsvc/gold/point_redshift/year={year}/month={month}/day={day}/hour={hour}
  file_key: pointsvc_g_point_redshift
  file_extension: parquet

  database: "{{ var.value.get('server_env', 'prod') }}_pointsvc"
  table: g_point_redshift
  location: s3://{{ var.value.get('server_env', 'prod') }}-buzzvil-data-lake/pointsvc/gold/point_redshift
  partition:
    name: partition_timestamp
    value: "{{ (execution_date - macros.timedelta(hours=2)).strftime('%Y-%m-%d %H:00:00') }}"
    location: s3://{{{{ var.value.get('server_env', 'prod') }}}}-buzzvil-data-lake/pointsvc/gold/point_redshift/year={year}/month={month}/day={day}/hour={hour}
  create_table_syntax: |
    CREATE EXTERNAL TABLE IF NOT EXISTS {database}.{table} (
      resource_id           BIGINT,
      resource_type         VARCHAR(16),
      event_type            VARCHAR(16),
      redeem_id             VARCHAR(72),
      redeem_type           VARCHAR(16),
      redeem_status         VARCHAR(16),

      amount                BIGINT,
      deposit_sum           BIGINT,
      withdraw_sum          BIGINT,

      account_id            BIGINT,
      app_id                BIGINT,
      publisher_user_id     VARCHAR(100),

      reward_transaction_id VARCHAR(72),
      unit_id               BIGINT,
      extra                 VARCHAR(65535),
      created_at            TIMESTAMP,
      updated_at            TIMESTAMP,

      version               BIGINT,
      scatter               BIGINT
    )
    PARTITIONED BY (partition_timestamp timestamp)
    STORED AS PARQUET
    LOCATION '{location}'
    tblproperties("parquet.compress"="SNAPPY");
