# athena_process_statssvc_g_unit_point read 6hours old data
#
# CREATE TABLE `unit_point` (
#   `id` int(11) NOT NULL AUTO_INCREMENT,
#   `data_at` datetime(6) NOT NULL,
#   `unit_id` bigint(20) NOT NULL,
#   `point_type` varchar(10) NOT NULL,
#   `unit_organization_id` int(11) NOT NULL,
#   `campaign_organization_id` int(11) NOT NULL,
#   `reward_count` int(10) unsigned NOT NULL,
#   `reward` int(11) NOT NULL,
#   `updated_at` datetime(6) NOT NULL,
#   `created_at` datetime(6) NOT NULL,
#   PRIMARY KEY (`id`),
#   UNIQUE KEY `unique_unit_point` (`data_at`,`unit_id`,`point_type`,`campaign_organization_id`)
# ) ENGINE=InnoDB AUTO_INCREMENT=3848353 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci
---
pipeline_key: statssvc_g_unit_point
pipeline_type: athena_process
pipeline_dag_configs:
  start_date: 2020-07-21 12:00:00
  schedule_interval: "0 * * * *"

alerts:
  slack:
    - trigger: failure
      args:
        channel: data-emergency
    - trigger: sla_miss
      args:
        channel: data-emergency
    - trigger: retry
      args:
        channel: data-warning

# 6 hours delayed
execution_delay: 21600

# latest_snapshot은 upstream dag로 두지 않음
upstream_dependencies:
  - dag_id: athena_view_bs_g_vw_point
    timedelta_hours: 0

downstream_dependencies:
  - dag_id: check_data_athena_unit_point__redshift_stat_point
    task_id: generate_uuid
# TODO Add downstream dependency
#   - dag_id: statssvc_import_job
#     task_id: unit_point_task

sensor_soft_fail: false

athena:
  workgroup: statssvc
  # https://github.com/Buzzvil/buzzscreen/blob/a0236d6e0ec75949c543bc31654e4ca9d048468e/statistics/management/commands/update_statistics_point.py#L79
  process_query: |
    CREATE TABLE IF NOT EXISTS {database}.{temp_table}
    WITH (
      format = 'PARQUET',
      parquet_compression = 'SNAPPY',
      external_location = 's3://{output_bucket}/{output_prefix}'
    ) AS (
      SELECT -- non base reward
        date_trunc('hour', P.created_at)                            AS data_at,
        P.unit_id                                                   AS unit_id,
        P.point_type                                                AS point_type,
        U.organization_id                                           AS unit_organization_id,
        COALESCE(C.organization_id, COALESCE(L.organization_id, 1)) AS campaign_organization_id,
        COUNT(*)                                                    AS reward_count,
        SUM(P.amount - P.base_reward)                               AS reward
      FROM {{{{ var.value.get('server_env', 'prod') }}}}_buzzscreen.g_vw_point P
      LEFT JOIN {{{{ var.value.get('server_env', 'prod') }}}}_buzzad.l_vw_unit U ON P.unit_id = U.id
      LEFT JOIN {{{{ var.value.get('server_env', 'prod') }}}}_buzzad.l_vw_lineitem L ON (P.campaign_id > 1000000000 AND P.campaign_id - 1000000000 = L.id)
      LEFT JOIN {{{{ var.value.get('server_env', 'prod') }}}}_buzzscreen.l_vw_content_campaigns C ON (P.campaign_id <= 1000000000 AND P.campaign_id = C.id)
      WHERE
        P.partition_timestamp >= TIMESTAMP '{start_time}' AND
        P.partition_timestamp <  TIMESTAMP '{end_time}' AND
        P.created_at >= TIMESTAMP '{start_time}' AND
        P.created_at <  TIMESTAMP '{end_time}' AND
        P.requested = TRUE AND
        P.amount > P.base_reward
      GROUP BY 1, 2, 3, 4, 5

      UNION ALL

      SELECT -- base reward
        date_trunc('hour', P.created_at)  AS data_at,
        P.unit_id                         AS unit_id,
        'base'                            AS point_type,
        U.organization_id                 AS unit_organization_id,
        1                                 AS campaign_organization_id,
        COUNT(*)                          AS reward_count,
        SUM(P.base_reward)                AS reward
      FROM {{{{ var.value.get('server_env', 'prod') }}}}_buzzscreen.g_vw_point P
      LEFT JOIN {{{{ var.value.get('server_env', 'prod') }}}}_buzzad.l_vw_unit U ON P.unit_id = U.id  -- TODO vw unit
      WHERE
        P.partition_timestamp >= TIMESTAMP '{start_time}' AND
        P.partition_timestamp <  TIMESTAMP '{end_time}' AND
        P.created_at >= TIMESTAMP '{start_time}' AND
        P.created_at <  TIMESTAMP '{end_time}' AND
        P.requested = TRUE
      GROUP BY 1, 2, 3, 4, 5
    )
  output_bucket: "{{ var.value.get('server_env', 'prod') }}-buzzvil-data-lake"
  output_prefix: statssvc/gold/unit_point/year={year}/month={month}/day={day}/hour={hour}
  file_key: statssvc_g_unit_point
  file_extension: parquet

  database: "{{ var.value.get('server_env', 'prod') }}_statssvc"
  table: g_unit_point
  location: "s3://{{ var.value.get('server_env', 'prod') }}-buzzvil-data-lake/statssvc/gold/unit_point"
  partition:
    name: partition_timestamp
    value: "{{ execution_date.strftime('%Y-%m-%d %H:00:00') }}"
    location: "s3://{{{{ var.value.get('server_env', 'prod') }}}}-buzzvil-data-lake/statssvc/gold/unit_point/year={year}/month={month}/day={day}/hour={hour}"
  create_table_syntax: |
    CREATE EXTERNAL TABLE IF NOT EXISTS {database}.{table} (
        data_at                   TIMESTAMP,
        unit_id                   BIGINT,
        point_type                VARCHAR(10),
        unit_organization_id      BIGINT,
        campaign_organization_id  BIGINT,
        reward_count              BIGINT,
        reward                    BIGINT
    )
    PARTITIONED BY (partition_timestamp timestamp)
    STORED AS PARQUET
    LOCATION '{location}'
    tblproperties("parquet.compress"="SNAPPY");
