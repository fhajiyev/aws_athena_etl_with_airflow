---
pipeline_key: segmentsvc_l_property_pixelsvc_data
pipeline_type: athena_process
pipeline_dag_configs:
  start_date: 2020-10-22 00:00:00
  schedule_interval: "0 * * * *"

alerts:
  slack:
    - trigger: failure
      args:
        channel: dev-emergency-mugshot
    - trigger: sla_miss
      args:
        channel: dev-emergency-mugshot
    - trigger: retry
      args:
        channel: data-warning

upstream_dependencies:
  - dag_id: athena_catalog_pixelsvc_l_track
    timedelta_hours: 0
downstream_dependencies:
  - dag_id: athena_process_segmentsvc_g_property_pixelsvc_data
    task_id: generate_uuid

athena:
  workgroup: segmentsvc
  process_query: |
    CREATE TABLE IF NOT EXISTS {database}.{temp_table}
    WITH (
      format = 'PARQUET',
      parquet_compression = 'SNAPPY',
      external_location = 's3://{output_bucket}/{output_prefix}'
    ) AS (
    SELECT
        CAST(payload['dsid'] AS BIGINT) AS data_source_id,
        log_at AS property_timestamp,
        payload AS payload,
        id_pair[1] AS uid_key,
        id_pair[2] AS uid_value
    FROM
    (

      SELECT
          ARRAY[
          ARRAY['CookieID',    payload['cookie_id']],
          ARRAY['AccountID',   payload['account_id']],
          ARRAY['IFA',         payload['ifa']],
          ARRAY[CONCAT(payload['dsid'], ':', 'UserID'), payload['user_id']],
          ARRAY['Email',       payload['email']]
          ] AS id_pairs,
          payload,
          log_at
      FROM
        {{{{ var.value.get('server_env', 'prod') }}}}_pixelsvc.l_track
      WHERE
        partition_timestamp >= TIMESTAMP'{start_time}' AND
        partition_timestamp < TIMESTAMP'{end_time}' AND
        payload['type'] = 'property' AND
        payload['dsid'] <> 'tech-buzzvil' AND payload['dsid'] <> 'local-test'
    )

    CROSS JOIN UNNEST(id_pairs) AS t(id_pair)
    WHERE
    id_pair[1] IS NOT null AND id_pair[1] <> '' AND
    id_pair[2] IS NOT null AND id_pair[2] <> ''

    );
  output_bucket: "{{ var.value.get('server_env', 'prod') }}-buzzvil-data-lake"
  output_prefix: "segmentsvc/landing/property/year={year}/month={month}/day={day}/hour={hour}/origin=pixelsvc_property"
  file_key: pixelsvc_property
  file_extension: parquet

  create_table_syntax: |
    CREATE EXTERNAL TABLE IF NOT EXISTS {database}.{table} (
      `data_source_id` BIGINT,
      `property_timestamp` TIMESTAMP,
      `payload` MAP<STRING,STRING>,
      `uid_key` STRING,
      `uid_value` STRING
    )
    PARTITIONED BY (
      partition_timestamp TIMESTAMP
    )
    STORED AS PARQUET
    LOCATION '{location}'
    TBLPROPERTIES("parquet.compress"="SNAPPY")
    ;
  database: "{{ var.value.get('server_env', 'prod') }}_segmentsvc"
  table: l_property_pixelsvc_data
  location: "s3://{{ var.value.get('server_env', 'prod') }}-buzzvil-data-lake/segmentsvc/landing/property/"
  partition:
    name: partition_timestamp
    value: "{{ execution_date.strftime('%Y-%m-%d %H:00:00') }}"
    location: "s3://{{{{ var.value.get('server_env', 'prod') }}}}-buzzvil-data-lake/segmentsvc/landing/property/year={year}/month={month}/day={day}/hour={hour}/origin=pixelsvc_property"